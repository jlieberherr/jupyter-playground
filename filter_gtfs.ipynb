{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter gtfs-feed\n",
    "A notebook with which timetables from a gtfs-feed can be filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input gtfs-feed\n",
    "input_gtfs = r\"D:\\data\\90_divers\\gtfsfp20192018-12-05.zip\"\n",
    "\n",
    "# a temporary gtfs-feed coded in utf-8 and not in utf-8-sig\n",
    "# the gtfs-feeds from https://opentransportdata.swiss/de/ are coded utf-8 with bom\n",
    "output_gtfs_without_bom = r\"D:\\data\\90_divers\\gtfsfp20192018-12-05_without_bom.zip\"\n",
    "\n",
    "# the gtfs-feed after filtering\n",
    "output_gtfs_small = r\"D:\\data\\90_divers\\gtfsfp20192018-12-05_small.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change encoding\n",
    "Write input-gtfs-feed from utf-8-sig to utf-8.\n",
    "This step is only necessary if the input gtfs-feed is not in utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tmp_dict = os.path.join(os.path.dirname(output_gtfs_without_bom), \"tmp\")\n",
    "if os.path.exists(path_tmp_dict) and os.path.isdir(path_tmp_dict):\n",
    "    shutil.rmtree(path_tmp_dict)\n",
    "temp_dict = os.mkdir(path_tmp_dict)\n",
    "with zipfile.ZipFile(input_gtfs, \"r\") as f:\n",
    "    for filename in f.namelist():\n",
    "        data = f.read(filename).decode(\"utf-8-sig\").encode(\"utf-8\")\n",
    "        tmp_filename = os.path.join(path_tmp_dict, filename)\n",
    "        with open(tmp_filename, \"w\") as tmp:\n",
    "            tmp.write(data)\n",
    "shutil.make_archive(os.path.splitext(output_gtfs_without_bom)[0], \"zip\", path_tmp_dict)\n",
    "shutil.rmtree(path_tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter gtfs-feed\n",
    "Given a list of routes we filter all information in the input-gtfs-feed which is connected to one of these routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the routes\n",
    "route_ids = [\"1-85-j19-1\", \"61-265-Y-j19-1\", \"26-759-j19-1\", \"90-73-Y-j19-1\", \"6-1-j19-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gtfs_feed(input_gtfs_feed, output_gtfs_feed, routes_to_filter):\n",
    "    # unzip gtfs-feed\n",
    "    dir_unzipped = os.path.splitext(input_gtfs_feed)[0]\n",
    "    with zipfile.ZipFile(input_gtfs_feed) as f:\n",
    "        f.extractall(dir_unzipped)\n",
    "\n",
    "    # create temporary directory\n",
    "    path_tmp_dict = os.path.join(os.path.dirname(output_gtfs_without_bom), \"tmp\")\n",
    "    if os.path.exists(path_tmp_dict) and os.path.isdir(path_tmp_dict):\n",
    "        shutil.rmtree(path_tmp_dict)\n",
    "    os.mkdir(path_tmp_dict)\n",
    "    \n",
    "    def filter_by_id(key, values, folder, feed_file, new_keys, out_folder):\n",
    "        with open(os.path.join(folder, feed_file)) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            res = []\n",
    "            new_values = []\n",
    "            for l in reader:\n",
    "                if l[key] in values:\n",
    "                    res += [l]\n",
    "                    new_values += [[l[new_key] for new_key in new_keys]]\n",
    "            with open(os.path.join(out_folder, feed_file), \"wb\") as g:\n",
    "                g.write(\",\".join(reader.fieldnames) + \"\\r\\n\")\n",
    "                writer = csv.DictWriter(g, fieldnames=reader.fieldnames, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                for l in res:\n",
    "                    writer.writerow(l)\n",
    "        return [set(x) for x in zip(*new_values)]\n",
    "    \n",
    "    trip_ids, service_ids = filter_by_id(\"route_id\", route_ids, dir_unzipped, \"trips.txt\", [\"trip_id\", \"service_id\"], path_tmp_dict)\n",
    "    stop_ids = filter_by_id(\"trip_id\", trip_ids, dir_unzipped, \"stop_times.txt\", [\"stop_id\"], path_tmp_dict)[0]\n",
    "    filter_by_id(\"stop_id\", stop_ids, dir_unzipped, \"stops.txt\", [], path_tmp_dict)\n",
    "    filter_by_id(\"service_id\", service_ids, dir_unzipped, \"calendar.txt\", [], path_tmp_dict)\n",
    "    filter_by_id(\"service_id\", service_ids, dir_unzipped, \"calendar_dates.txt\", [], path_tmp_dict)\n",
    "    agency_ids = filter_by_id(\"route_id\", route_ids, dir_unzipped, \"routes.txt\", [\"agency_id\"], path_tmp_dict)[0]\n",
    "    filter_by_id(\"agency_id\", agency_ids, dir_unzipped, \"agency.txt\", [], path_tmp_dict)\n",
    "    \n",
    "    def filter_transfers(stop_ids, folder, out_folder):\n",
    "        with open(os.path.join(folder, \"transfers.txt\")) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            res = []\n",
    "            for l in reader:\n",
    "                if l[\"from_stop_id\"] in stop_ids and l[\"to_stop_id\"] in stop_ids:\n",
    "                    res += [l]\n",
    "            with open(os.path.join(out_folder, \"transfers.txt\"), \"wb\") as g:\n",
    "                g.write(\",\".join(reader.fieldnames) + \"\\r\\n\")\n",
    "                writer = csv.DictWriter(g, fieldnames=reader.fieldnames, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                for l in res:\n",
    "                    writer.writerow(l)\n",
    "    \n",
    "    filter_transfers(stop_ids, dir_unzipped, path_tmp_dict)\n",
    "    \n",
    "    shutil.copyfile(os.path.join(dir_unzipped, \"feed_info.txt\"), os.path.join(path_tmp_dict, \"feed_info.txt\"))\n",
    "    \n",
    "    # zip it\n",
    "    shutil.make_archive(os.path.splitext(output_gtfs_small)[0], \"zip\", path_tmp_dict)\n",
    "    \n",
    "    # remove tmp-data\n",
    "    if os.path.exists(path_tmp_dict) and os.path.isdir(path_tmp_dict):\n",
    "        shutil.rmtree(path_tmp_dict)\n",
    "    if os.path.exists(dir_unzipped) and os.path.isdir(dir_unzipped):\n",
    "        shutil.rmtree(dir_unzipped)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_gtfs_feed(output_gtfs_without_bom, output_gtfs_small, route_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
